{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50a6e0-338d-4fba-b4c5-7477bb0c56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot-class/.config/matplotlib is not a writable directory\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-7prepkj9 because there was an issue with the default path (/home/iot-class/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# %pip install git+https://github.com/JiahuiYu/neuralgym\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CARLA_DATA = False\n",
    "\n",
    "dataset_dir = \"datasets\"\n",
    "\n",
    "if CARLA_DATA:\n",
    "    num_views = 1\n",
    "    num_frames = 179\n",
    "    dataset_idx = 0\n",
    "    MASK_SPAN = {\"face\":0.2, \"no_feet\":.85, \"full\":1}\n",
    "    X = 2  # chunk size\n",
    "    mask_height_span = MASK_SPAN.get('face', 1)\n",
    "    output_base_directory = \"output/carla_v1\"\n",
    "    input_video_base_path = f\"output/carla_v1/rgb/view\"\n",
    "    video_output_dir = \"output/carla_v1/videos/\"\n",
    "else:\n",
    "    num_views = 2\n",
    "    num_frames = 50\n",
    "    dataset_idx = 1\n",
    "    MASK_SPAN = {\"face\":0.2, \"no_feet\":.9, \"full\":1}\n",
    "    X = 4  # chunk size\n",
    "    mask_height_span = MASK_SPAN.get('no_feet', 1)\n",
    "    output_base_directory = \"jc_8_long_n2\"\n",
    "    input_video_base_path = f\"output/xr_lubna/rgb/view\"\n",
    "    video_output_dir = \"output/xr_lubna/videos/\"\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        valid_views = [f\"view_{i}\" for i in range(num_views)]\n",
    "        valid_frames = range(num_frames)  # frames 0 to 49\n",
    "        valid_image_names = [f\"pointcloud-{i}.png\" for i in valid_frames]\n",
    "        valid_depth_names = [f\"depth-{i}.png\" for i in valid_frames]\n",
    "\n",
    "        # We'll store data per scenario like:\n",
    "        # self.scenarios = [\n",
    "        #   {\n",
    "        #       \"images\": [[img_view0_frame0, ...], [img_view1_frame0, ...], ...], # [8][50]\n",
    "        #       \"gts\":    [[gt_view0_frame0, ...], ...],\n",
    "        #       \"depths\": [[depth_view0_frame0, ...], ...],\n",
    "        #       \"paths\":  [[path_view0_frame0, ...], ...]\n",
    "        #   }, ...\n",
    "        # ]\n",
    "        self.scenarios = []\n",
    "\n",
    "        # Identify scenario folders (excluding ground truth)\n",
    "        all_folders = [f for f in os.listdir(root_dir) \n",
    "                       if os.path.isdir(os.path.join(root_dir, f)) and \n",
    "                       not f.endswith('ground_truth') and '_ground_truth' not in f]\n",
    "\n",
    "        for folder in all_folders:\n",
    "            gt_folder = f\"{folder}_ground_truth\"\n",
    "            if not os.path.exists(os.path.join(root_dir, gt_folder)):\n",
    "                continue  # skip if no corresponding ground truth folder\n",
    "\n",
    "\n",
    "            scenario_images = []\n",
    "            scenario_gts = []\n",
    "            scenario_depths = []\n",
    "            scenario_paths = []\n",
    "\n",
    "            # For each view\n",
    "            for view_folder in valid_views:\n",
    "                image_folder_path = os.path.join(root_dir, folder, view_folder)\n",
    "                gt_folder_path = os.path.join(root_dir, gt_folder, view_folder)\n",
    "\n",
    "                if not (os.path.exists(image_folder_path) and os.path.exists(gt_folder_path)):\n",
    "                    scenario_images = []\n",
    "                    break\n",
    "\n",
    "                view_images = []\n",
    "                view_gts = []\n",
    "                view_depths = []\n",
    "                view_paths = []\n",
    "\n",
    "                # Make sure frames are in order\n",
    "                for frame_idx in valid_frames:\n",
    "                    image_name = f\"pointcloud-{frame_idx}.png\"\n",
    "                    depth_name = f\"depth-{frame_idx}.png\"\n",
    "                    image_path = os.path.join(image_folder_path, image_name)\n",
    "                    depth_path = os.path.join(image_folder_path, depth_name)\n",
    "                    gt_image_path = os.path.join(gt_folder_path, image_name)\n",
    "\n",
    "                    if not (os.path.exists(image_path) and os.path.exists(gt_image_path) and os.path.exists(depth_path)):\n",
    "                        view_images = []\n",
    "                        break\n",
    "\n",
    "                    view_images.append(image_path)\n",
    "                    view_gts.append(gt_image_path)\n",
    "                    view_depths.append(depth_path)\n",
    "\n",
    "                    relative_path = image_path.replace(self.root_dir + \"/\", \"\")\n",
    "                    view_paths.append(relative_path)\n",
    "\n",
    "                # If any frame missing, break\n",
    "                if len(view_images) != num_frames:\n",
    "                    scenario_images = []\n",
    "                    break\n",
    "\n",
    "                scenario_images.append(view_images)\n",
    "                scenario_gts.append(view_gts)\n",
    "                scenario_depths.append(view_depths)\n",
    "                scenario_paths.append(view_paths)\n",
    "\n",
    "            # Add the scenario if all views and frames loaded\n",
    "            if len(scenario_images) == num_views and all(len(v) == num_frames for v in scenario_images):\n",
    "                self.scenarios.append({\n",
    "                    \"images\": scenario_images,\n",
    "                    \"gts\": scenario_gts,\n",
    "                    \"depths\": scenario_depths,\n",
    "                    \"paths\": scenario_paths\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        print(f\"Total scenarios loaded: {len(self.scenarios)}\")\n",
    "        return len(self.scenarios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scenario = self.scenarios[idx]\n",
    "        scenario_images = scenario[\"images\"]   # [8][50]\n",
    "        scenario_gts = scenario[\"gts\"]         # [8][50]\n",
    "        scenario_depths = scenario[\"depths\"]   # [8][50]\n",
    "        scenario_paths = scenario[\"paths\"]     # [8][50]\n",
    "\n",
    "        # We'll load and process all images and masks\n",
    "        all_images = []  # Will hold [8, 50, 4, H, W] eventually\n",
    "        all_masks = []   # Will hold [8, 50, H, W]\n",
    "\n",
    "        # Define class colors\n",
    "        class_1_color = np.array([80, 239, 7])   # #50EF07\n",
    "        class_2_color = np.array([249, 0, 0])    # #F90000\n",
    "        tolerance = 30\n",
    "\n",
    "        for v in range(num_views):\n",
    "            view_imgs = []\n",
    "            view_masks = []\n",
    "            # For each frame in this view\n",
    "            for f in range(num_frames):\n",
    "                image_path = scenario_images[v][f]\n",
    "                gt_path = scenario_gts[v][f]\n",
    "                depth_path = scenario_depths[v][f]\n",
    "\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                gt_image = Image.open(gt_path).convert(\"RGB\")\n",
    "                depth_image = Image.open(depth_path)\n",
    "\n",
    "                gt_image_np = np.array(gt_image)\n",
    "                # Create mask\n",
    "                mask = np.zeros(gt_image_np.shape[:2], dtype=np.uint8)\n",
    "                mask[np.all(np.abs(gt_image_np - class_1_color) <= tolerance, axis=-1)] = 1\n",
    "                mask[np.all(np.abs(gt_image_np - class_2_color) <= tolerance, axis=-1)] = 2\n",
    "\n",
    "                # Convert images to tensors and apply transform if provided\n",
    "                if self.transform:\n",
    "                    # Apply transform to RGB image\n",
    "                    rgb_tensor = self.transform(image)  # [C,H,W]\n",
    "                    # Resize mask using nearest neighbor\n",
    "                    mask_pil = Image.fromarray(mask)\n",
    "                    mask_pil = mask_pil.resize((rgb_tensor.shape[2], rgb_tensor.shape[1]), Image.NEAREST)\n",
    "                    mask = np.array(mask_pil)\n",
    "\n",
    "                    # Resize depth image separately\n",
    "                    depth_resized = depth_image.resize((rgb_tensor.shape[2], rgb_tensor.shape[1]), Image.NEAREST)\n",
    "                    depth_np = np.array(depth_resized).astype(np.float32)\n",
    "\n",
    "                else:\n",
    "                    # If no transform, just convert directly\n",
    "                    rgb_tensor = transforms.ToTensor()(image)\n",
    "                    depth_np = np.array(depth_image).astype(np.float32)\n",
    "\n",
    "                # Normalize depth\n",
    "                # if depth_np.max() > 10 * depth_np.min():\n",
    "                #     depth_norm = depth_np/1000\n",
    "                # else:\n",
    "                #     depth_norm = depth_np  # all pixels same, no normalization needed\n",
    "                # depth_norm = depth_np/1000\n",
    "                depth_norm = depth_np\n",
    "\n",
    "                \n",
    "                depth_tensor = torch.tensor(depth_norm).unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "                # Combine RGB and Depth into single tensor: [4,H,W]\n",
    "                img_with_depth = torch.cat((rgb_tensor, depth_tensor), dim=0)\n",
    "\n",
    "                # Convert mask to tensor\n",
    "                mask = torch.from_numpy(mask).long()\n",
    "\n",
    "                view_imgs.append(img_with_depth)  # [4,H,W]\n",
    "                view_masks.append(mask)           # [H,W]\n",
    "\n",
    "            # Stack frames for this view\n",
    "            view_imgs = torch.stack(view_imgs, dim=0)   # [50,4,H,W]\n",
    "            view_masks = torch.stack(view_masks, dim=0) # [50,H,W]\n",
    "\n",
    "            all_images.append(view_imgs)\n",
    "            all_masks.append(view_masks)\n",
    "\n",
    "        # Stack all views\n",
    "        all_images = torch.stack(all_images, dim=0)  # [8,50,4,H,W]\n",
    "        all_masks = torch.stack(all_masks, dim=0)    # [8,50,H,W]\n",
    "\n",
    "        return all_images, all_masks, scenario_paths\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d38fb3c-ae58-41a6-a358-dcdbadca1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scenarios loaded: 12\n",
      "Total scenarios loaded: 12\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = SegmentationDataset(root_dir=dataset_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0dea16a-0bb8-4180-8cee-e9b1b1fc9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, paths = dataset[dataset_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0dbd303-ff27-4d09-ac3b-8f8589ffaf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-0.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-1.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-2.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-3.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-4.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-5.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-6.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-7.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-8.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-9.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-10.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-11.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-12.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-13.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-14.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-15.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-16.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-17.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-18.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-19.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-20.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-21.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-22.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-23.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-24.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-25.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-26.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-27.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-28.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-29.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-30.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-31.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-32.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-33.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-34.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-35.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-36.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-37.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-38.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-39.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-40.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-41.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-42.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-43.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-44.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-45.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-46.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-47.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-48.png', 'LubnaFriends_redandblack-0.0015_720p_mobility_linear_minus_0_4_mps_total_dist_8/view_0/pointcloud-49.png']\n"
     ]
    }
   ],
   "source": [
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b8a22-0254-4d90-935d-9548110b6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot-class/Capstone/3d-privacy-masking/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/iot-class/Capstone/3d-privacy-masking/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "# ---------------------------\n",
    "# Global constants\n",
    "# ---------------------------\n",
    "DETECTION_CONFIDENCE_THRESHOLD = 0.65\n",
    "PRIVATE_OBJECT_CLASSES = ['person']\n",
    "DEPTH_THRESHOLD_MULTIPLIER = 75\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella',\n",
    "    'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "    'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass', 'cup', 'fork',\n",
    "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n",
    "    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', 'toilet',\n",
    "    'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "    'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book', 'clock', 'vase',\n",
    "    'scissors', 'teddy bear', 'hair dryer', 'toothbrush'\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "###############################################################################\n",
    "# 1) DETECTION\n",
    "###############################################################################\n",
    "def detect_objects(model, image_np, confidence_threshold=0.5, draw_boxes=False):\n",
    "    \"\"\"\n",
    "    image_np: NumPy array [H,W,3], float in [0,1] (on CPU).\n",
    "    Model forward pass on GPU. Returns bounding boxes on CPU.\n",
    "    \"\"\"\n",
    "    transform_ = transforms.Compose([transforms.ToTensor()])\n",
    "    input_tensor = transform_(image_np).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs]\n",
    "    scores = outputs[0]['scores'].numpy()\n",
    "    labels = outputs[0]['labels'].numpy()\n",
    "    boxes = outputs[0]['boxes'].numpy()\n",
    "\n",
    "    objects = []\n",
    "    for score, label_idx, box in zip(scores, labels, boxes):\n",
    "        if score >= confidence_threshold:\n",
    "            label = COCO_INSTANCE_CATEGORY_NAMES[label_idx]\n",
    "            x1, y1, x2, y2 = box.astype(int)\n",
    "            objects.append({\n",
    "                'box': [x1, y1, x2, y2],\n",
    "                'score': float(score),\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "    if draw_boxes:\n",
    "        # Create a displayable image directly from the input image_np.\n",
    "        # This ensures the base image colors and exposure are preserved from your original.\n",
    "        # Scale to 0-255 and convert to uint8.\n",
    "        # display_image = (image_np * 255).astype(np.uint8)\n",
    "        display_image = image_np.copy()  # Use the original image_np directly\n",
    "\n",
    "        # Convert the display_image from RGB to BGR, which is OpenCV's default format\n",
    "        # for drawing and saving. This step is crucial for accurate color representation\n",
    "        # when using cv2.rectangle and cv2.putText, and for cv2.imwrite.\n",
    "        # display_image = cv2.cvtColor(display_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            label = obj['label']\n",
    "            score = obj['score']\n",
    "\n",
    "            if label not in PRIVATE_OBJECT_CLASSES:\n",
    "                continue\n",
    "            \n",
    "            # Define text properties\n",
    "            text = f'{label}: {score:.2f}'\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.5\n",
    "            font_thickness = 1\n",
    "            \n",
    "            # Get text size to position it properly\n",
    "            text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
    "            \n",
    "            # Position text above the bounding box. Adjust if it goes off the top of the image.\n",
    "            text_x = x1\n",
    "            text_y = y1 - 10 if y1 - 10 > text_size[1] else y1 + text_size[1] + 5 # Add 5 for slight padding\n",
    "\n",
    "            # Draw rectangle on the image\n",
    "            # Color is green (0, 255, 0) in BGR\n",
    "            cv2.rectangle(display_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Put text on the image\n",
    "            cv2.putText(display_image, text, (text_x, text_y), font, font_scale, (0, 255, 0), font_thickness, cv2.LINE_AA)\n",
    "            \n",
    "        return display_image\n",
    "\n",
    "    else:\n",
    "        return objects\n",
    "    \n",
    "###############################################################################\n",
    "# 2) DEPTH PROFILE\n",
    "###############################################################################\n",
    "def calculate_depth_profile_of_box(depth_map, x1, y1, x2, y2, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Return { 'mean','std','threshold','box':[x1,y1,x2,y2] } or None if empty.\n",
    "    \"\"\"\n",
    "    half_window = window_size // 2\n",
    "    cx = (x1 + x2) // 2\n",
    "    cy = (y1 + y2) // 2\n",
    "\n",
    "    x_start = max(cx - half_window, 0)\n",
    "    x_end = min(cx + half_window + 1, depth_map.shape[1])\n",
    "    y_start = max(cy - half_window, 0)\n",
    "    y_end = min(cy + half_window + 1, depth_map.shape[0])\n",
    "\n",
    "    depth_window = depth_map[y_start:y_end, x_start:x_end]\n",
    "    depth_values = depth_window.flatten()\n",
    "    depth_values = depth_values[~np.isnan(depth_values)]\n",
    "    if depth_values.size == 0:\n",
    "        return None\n",
    "\n",
    "    depth_mean = float(np.mean(depth_values))\n",
    "    depth_std = float(np.std(depth_values))\n",
    "    depth_threshold = float(depth_std * DEPTH_THRESHOLD_MULTIPLIER)\n",
    "\n",
    "    return {\n",
    "        'mean': depth_mean,\n",
    "        'std': depth_std,\n",
    "        'threshold': depth_threshold,\n",
    "        'box': [x1,y1,x2,y2]\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 3) SEGMENT CHUNK\n",
    "###############################################################################\n",
    "\n",
    "def segment_person_from_box(depth_tensor, dprof, span=1):\n",
    "    \"\"\"\n",
    "    Similar to segment_person_from_profile_batch, but for a single bounding box\n",
    "    in just 1 frame's depth or multiple frames (X frames).\n",
    "    If depth_tensor: shape [X,H,W] or [H,W].\n",
    "    \"\"\"\n",
    "    if len(depth_tensor.shape) == 2:\n",
    "        # single frame => shape [H,W]\n",
    "        depth_tensor = depth_tensor.unsqueeze(0)  # => [1,H,W]\n",
    "\n",
    "    depth_mean = torch.tensor(dprof['mean'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    depth_thr  = torch.tensor(dprof['threshold'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    (x1,y1,x2,y2) = dprof['box']\n",
    "    y2 = int(y1 + span * (y2 - y1))\n",
    "\n",
    "\n",
    "    depth_diff = torch.abs(depth_tensor - depth_mean)\n",
    "    mask_batch = (depth_diff <= depth_thr).to(torch.uint8)\n",
    "\n",
    "    final_mask = torch.zeros_like(mask_batch)\n",
    "    _, H, W = depth_tensor.shape\n",
    "    x1_clamp = max(0, min(x1,W))\n",
    "    x2_clamp = max(0, min(x2,W))\n",
    "    y1_clamp = max(0, min(y1,H))\n",
    "    y2_clamp = max(0, min(y2,H))\n",
    "\n",
    "    if x2_clamp> x1_clamp and y2_clamp> y1_clamp:\n",
    "        final_mask[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp] = \\\n",
    "            mask_batch[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp]\n",
    "\n",
    "    # return shape [H,W] if single frame\n",
    "    if final_mask.shape[0] == 1:\n",
    "        return final_mask[0]\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "def segment_all(depth_tensor, objects, depth_map, span):\n",
    "    \"\"\"\n",
    "    We create a combined mask of shape [H,W] = 1 for each person's bounding box,\n",
    "    EXCEPT we skip the public_box (which is the \"public\" person).\n",
    "    depth_tensor: shape [H,W], float on GPU\n",
    "    objects: detection results on CPU\n",
    "    public_box: (x1,y1,x2,y2) that we skip\n",
    "    depth_map: CPU 2D array for depth\n",
    "    Return: torch.uint8 mask [H,W], 1=private, 0=public\n",
    "    \"\"\"\n",
    "    H, W = depth_tensor.shape[-2], depth_tensor.shape[-1]\n",
    "\n",
    "    combined_mask = torch.zeros((H,W), dtype=torch.uint8, device=depth_tensor.device)\n",
    "\n",
    "    for obj in objects:\n",
    "        if obj['label'] not in PRIVATE_OBJECT_CLASSES:\n",
    "            continue\n",
    "        box = obj['box']  # [x1,y1,x2,y2]\n",
    "\n",
    "        # otherwise, segment this bounding box => \"private\"\n",
    "        dprof = calculate_depth_profile_of_box(depth_map, *box)\n",
    "        if dprof is None:\n",
    "            continue\n",
    "\n",
    "        single_mask = segment_person_from_box(depth_tensor, dprof, span)\n",
    "        combined_mask = torch.logical_or(combined_mask.bool(), single_mask.bool()).to(torch.uint8)\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "###############################################################################\n",
    "# 4) METRICS\n",
    "###############################################################################\n",
    "def dice_score_batch(pred_batch, gt_batch):\n",
    "    intersection = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    pred_sum = torch.sum(pred_batch==1).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if (pred_sum+gt_sum)==0:\n",
    "        return 1.0\n",
    "    return 2.0*intersection/(pred_sum+gt_sum)\n",
    "\n",
    "def recall_batch(pred_batch, gt_batch):\n",
    "    tp = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if gt_sum==0:\n",
    "        return 1.0\n",
    "    return tp/gt_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5bb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The following code is commented out because it was not used in the final implementation. \n",
    "It implements parallelization of chunk detection and segmentation.\n",
    "Ultimately, the approach had more overhead and gave worse times.'''\n",
    "\n",
    "# def chunk_detect_and_segment(start_f, X, images, num_views, num_frames, mask_height_span,):\n",
    "#     end_f = min(start_f + X, num_frames)\n",
    "#     pred_mask_part = torch.zeros((num_views, end_f-start_f, images.shape[-2], images.shape[-1]), dtype=torch.uint8, device=images.device)\n",
    "\n",
    "#     # (A) Re-detect \"private person\" in reference view\n",
    "    \n",
    "\n",
    "#     # rgb_ref_tensor = images[ref_view_idx, start_f, :3, :, :]\n",
    "#     # depth_ref_tensor = images[ref_view_idx, start_f, 3, :, :]\n",
    "\n",
    "#     # rgb_ref_np = rgb_ref_tensor.permute(1,2,0).cpu().numpy()  # => [H,W,3]\n",
    "#     # depth_ref_np = depth_ref_tensor.cpu().numpy()             # => [H,W]\n",
    "\n",
    "#     # # Detect all persons\n",
    "#     # objs_ref = detect_objects(model, rgb_ref_np, DETECTION_CONFIDENCE_THRESHOLD)\n",
    "\n",
    "#     # # Build list of depth profiles\n",
    "#     # profiles_ref = []\n",
    "#     # for obj in objs_ref:\n",
    "#     #     if obj['label'] in PRIVATE_OBJECT_CLASSES:\n",
    "#     #         (rx1,ry1,rx2,ry2) = obj['box']\n",
    "#     #         dprof = calculate_depth_profile_of_box(depth_ref_np, rx1, ry1, rx2, ry2)\n",
    "#     #         if dprof is not None:\n",
    "#     #             profiles_ref.append(dprof)\n",
    "#     # profiles_ref.sort(key=lambda x: x['mean'], reverse=True)\n",
    "#     # ref_profile=None\n",
    "#     # if len(profiles_ref)>=2:\n",
    "#     #     # second-furthest => index=0\n",
    "#     #     ref_profile = profiles_ref[0]\n",
    "\n",
    "#     # detection_time = time.time()-t0\n",
    "\n",
    "#     # (B) For each view, transform center & pick bounding box\n",
    "#     # seg_start_time = time.time()\n",
    "#     # center of ref bounding box\n",
    "#     # (rx1, ry1, rx2, ry2) = ref_profile['box']\n",
    "#     # rcx = 0.5*(rx1+rx2)\n",
    "#     # rcy = 0.5*(ry1+ry2)\n",
    "#     # rcz = ref_profile['mean']\n",
    "#     # ref_center_3d = (rcx, rcy, rcz)\n",
    "\n",
    "#     detection_times = []\n",
    "#     seg_times = []\n",
    "\n",
    "#     seg_start_time = time.time()\n",
    "#     # Now loop over all views\n",
    "#     for v in range(num_views):\n",
    "#         # 1) transform the reference center to the v-th view\n",
    "#         # (Tx, Ty, Tz) = transfer_point(v, ref_center_3d)\n",
    "\n",
    "#         # 2) detect persons in chunk's first frame for view v\n",
    "#         rgb_v_tensor = images[v, start_f, :3, :, :]\n",
    "#         depth_v_tensor = images[v, start_f, 3, :, :]\n",
    "\n",
    "#         rgb_v_np = rgb_v_tensor.detach().permute(1,2,0).cpu().numpy()\n",
    "#         depth_v_np = depth_v_tensor.detach().cpu().numpy().copy()\n",
    "\n",
    "#         t0 = time.time()\n",
    "\n",
    "#         objs_v = detect_objects(model, rgb_v_np, DETECTION_CONFIDENCE_THRESHOLD)\n",
    "\n",
    "#         detection_time = time.time()-t0\n",
    "\n",
    "\n",
    "#         # # 3) pick bounding box whose center is closest to (Tx,Ty,Tz)\n",
    "#         # chosen_box = pick_box_closest_3d(objs_v, depth_v_np, Tx, Ty, Tz)\n",
    "#         # if chosen_box is not None:\n",
    "#         #     dprof_v = calculate_depth_profile_of_box(depth_v_np, *chosen_box)\n",
    "#         #     if dprof_v is not None:\n",
    "#         #         # segment frames [start_f..end_f-1]\n",
    "#         #         depth_segment = images[v, start_f:end_f, 3, :, :]\n",
    "#         #         pred_segment = segment_person_from_profile_batch(depth_segment, dprof_v)\n",
    "#         #         pred_mask_full[v, start_f:end_f] = pred_segment\n",
    "\n",
    "#         # create mask => all persons except the public_box\n",
    "#         # shape => [H,W]\n",
    "\n",
    "#         # seg_start_time = time.time()\n",
    "\n",
    "#         depth_segment_1frame = images[v, start_f:end_f, 3, :, :]\n",
    "#         private_mask_1frame = segment_all(\n",
    "#             depth_segment_1frame,  # shape [H,W] on GPU\n",
    "#             objs_v,                # detections on CPU\n",
    "#             depth_v_np,             # CPU depth\n",
    "#             mask_height_span        # mask height cutoff\n",
    "#         )\n",
    "\n",
    "#         # If X>1, you'd do frames [start_f..end_f-1]. For X=1, it's just 1 frame\n",
    "#         # We'll store into pred_mask_full[v, start_f]\n",
    "\n",
    "#         seg_time = time.time() - seg_start_time\n",
    "#         print(f\"View {v}: Detection time: {detection_time:.4f}s, Segmentation time: {seg_time:.4f}s\")\n",
    "#         pred_mask_part[v, :] = private_mask_1frame\n",
    "#         detection_times.append(detection_time)\n",
    "#         seg_times.append(seg_time)\n",
    "        \n",
    "#     seg_start_time = time.time()\n",
    "\n",
    "    \n",
    "#     detection_times = sum(detection_times)\n",
    "#     seg_times = sum(seg_times)\n",
    "\n",
    "\n",
    "#     return detection_times, seg_times, pred_mask_part, start_f, end_f  # [num_views, X, H, W]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# # Suppose we load data\n",
    "# # images, masks, _ = dataset[0]\n",
    "# # images = images.to(device)\n",
    "# # masks = masks.to(device)\n",
    "\n",
    "# MASK_SPAN = {\"face\":0.2, \"no_feet\":.85, \"full\":1}\n",
    "\n",
    "\n",
    "# num_views = 2\n",
    "# num_frames = 50\n",
    "# X = 5  # chunk size\n",
    "# ref_view_idx = 7\n",
    "# mask_height_span = MASK_SPAN.get('face', 1)\n",
    "\n",
    "# # We allocate pred_mask_full => shape [num_views, num_frames, H, W]\n",
    "# pred_mask_full = torch.zeros(\n",
    "#     (num_views, num_frames, images.shape[-2], images.shape[-1]),\n",
    "#     dtype=torch.uint8,\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# chunk_detection_times = []\n",
    "# chunk_seg_times = []\n",
    "# chunk_total_times = []\n",
    "\n",
    "# # We'll define a list of chunk start indices\n",
    "# chunk_starts = list(range(0, num_frames, X))\n",
    "\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#     futures = {executor.submit(chunk_detect_and_segment, start_f, X, images, num_views, num_frames, mask_height_span): start_f\n",
    "#                 for start_f in chunk_starts}\n",
    "#     for future in as_completed(futures):\n",
    "#         detection_times, seg_times, pred_mask_part, start_f, end_f = future.result()\n",
    "\n",
    "#         pred_mask_full[:,start_f:end_f] = pred_mask_part\n",
    "\n",
    "#         chunk_detection_times.append(detection_times)\n",
    "#         chunk_seg_times.append(seg_times)\n",
    "#         chunk_total_times.append(detection_times + seg_times)\n",
    "\n",
    "# print(chunk_detection_times)\n",
    "# print(chunk_seg_times)\n",
    "# print(chunk_total_times)\n",
    "\n",
    "# # After all chunks, compute dice/recall per view\n",
    "# dice = 0\n",
    "# recall = 0\n",
    "# for v in range(num_views):\n",
    "#     gt_mask_view = (masks[v]==2).to(torch.uint8)\n",
    "#     dice_v = dice_score_batch(pred_mask_full[v].cpu(), gt_mask_view.cpu())\n",
    "#     recall_v = recall_batch(pred_mask_full[v].cpu(), gt_mask_view.cpu())\n",
    "#     dice += dice_v\n",
    "#     recall += recall_v\n",
    "#     print(f\"View {v}: Dice={dice_v:.4f}, Recall={recall_v:.4f}\")\n",
    "\n",
    "# # Print timing info\n",
    "# num_chunks = len(chunk_starts)\n",
    "# avg_det = sum(chunk_detection_times)/num_chunks\n",
    "# avg_seg = sum(chunk_seg_times)/num_chunks\n",
    "# avg_tot = sum(chunk_total_times)/num_chunks\n",
    "# print(f\"Avg detection time per chunk = {avg_det:.4f}s\")\n",
    "# print(f\"Avg segmentation time per chunk = {avg_seg:.4f}s\")\n",
    "# print(f\"Avg total time per chunk = {avg_tot:.4f}s\")\n",
    "# print(f\"Total time for all chunks = {sum(chunk_total_times):.4f}s\")\n",
    "# print(dice/num_views)\n",
    "# print(recall/num_views)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3452cd-4e47-49cc-b77d-d2f7af4945d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk starts: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48]\n",
      "[2.384185791015625e-07, 7.152557373046875e-07, 4.76837158203125e-07, 4.76837158203125e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 2.384185791015625e-07, 4.76837158203125e-07, 4.76837158203125e-07, 7.152557373046875e-07, 2.384185791015625e-07, 4.76837158203125e-07, 4.76837158203125e-07, 2.384185791015625e-07, 4.76837158203125e-07, 4.76837158203125e-07, 4.76837158203125e-07, 4.76837158203125e-07, 4.76837158203125e-07]\n",
      "[0.2231748104095459, 0.03838682174682617, 0.0379948616027832, 0.038097381591796875, 0.03976798057556152, 0.037738800048828125, 0.037686824798583984, 0.03810715675354004, 0.039350032806396484, 0.040148258209228516, 0.038753509521484375, 0.03995776176452637, 0.03848385810852051, 0.038254499435424805, 0.03876972198486328, 0.03988289833068848, 0.039734840393066406, 0.03893613815307617, 0.03873562812805176, 0.039222002029418945, 0.039664268493652344, 0.03863644599914551, 0.03969836235046387, 0.03873777389526367, 0.03916501998901367]\n",
      "[0.223175048828125, 0.03838753700256348, 0.037995338439941406, 0.03809785842895508, 0.039768218994140625, 0.03773903846740723, 0.037687063217163086, 0.03810739517211914, 0.039350271224975586, 0.04014849662780762, 0.03875374794006348, 0.03995800018310547, 0.03848409652709961, 0.03825497627258301, 0.038770198822021484, 0.03988361358642578, 0.03973507881164551, 0.038936614990234375, 0.03873610496520996, 0.03922224044799805, 0.03966474533081055, 0.03863692283630371, 0.03969883918762207, 0.038738250732421875, 0.039165496826171875]\n",
      "View 0: Dice=0.3066, Recall=0.8606\n",
      "View 1: Dice=0.4368, Recall=0.9367\n",
      "Avg detection time per chunk = 0.0000s\n",
      "Avg segmentation time per chunk = 0.0463s\n",
      "Avg total time per chunk = 0.0463s\n",
      "Total time for all chunks = 1.1571s\n",
      "0.3717034835945128\n",
      "0.8986395216494404\n"
     ]
    }
   ],
   "source": [
    "# We allocate pred_mask_full => shape [num_views, num_frames, H, W]\n",
    "pred_mask_full = torch.zeros(\n",
    "    (num_views, num_frames, images.shape[-2], images.shape[-1]),\n",
    "    dtype=torch.uint8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "chunk_detection_times = []\n",
    "chunk_seg_times = []\n",
    "chunk_total_times = []\n",
    "\n",
    "# We'll define a list of chunk start indices\n",
    "chunk_starts = list(range(0, num_frames, X))\n",
    "\n",
    "print(f\"Chunk starts: {chunk_starts}\")\n",
    "\n",
    "\n",
    "for start_f in chunk_starts:\n",
    "    end_f = min(start_f + X, num_frames)\n",
    "    seg_start_time = time.time()\n",
    "\n",
    "    # Now loop over all views\n",
    "    for v in range(num_views):\n",
    "\n",
    "        # 1) detect persons in chunk's first frame for view v\n",
    "        rgb_v_tensor = images[v, start_f, :3, :, :]\n",
    "        depth_v_tensor = images[v, start_f, 3, :, :]\n",
    "\n",
    "        rgb_v_np = rgb_v_tensor.detach().permute(1,2,0).cpu().numpy()\n",
    "        depth_v_np = depth_v_tensor.detach().cpu().numpy().copy()\n",
    "\n",
    "        objs_v = detect_objects(model, rgb_v_np, DETECTION_CONFIDENCE_THRESHOLD)\n",
    "\n",
    "        # 2) create mask\n",
    "        # shape => [H,W]\n",
    "        depth_segment_1frame = images[v, start_f:end_f, 3, :, :]\n",
    "        private_mask_1frame = segment_all(\n",
    "            depth_segment_1frame,  # shape [H,W] on GPU\n",
    "            objs_v,                # detections on CPU\n",
    "            depth_v_np,             # CPU depth\n",
    "            mask_height_span        # mask height cutoff\n",
    "        )\n",
    "\n",
    "        # If X>1, you'd do frames [start_f..end_f-1]. For X=1, it's just 1 frame\n",
    "        # We'll store into pred_mask_full[v, start_f]\n",
    "        pred_mask_full[v, start_f:end_f] = private_mask_1frame\n",
    "\n",
    "\n",
    "    seg_time = time.time() - seg_start_time\n",
    "\n",
    "    chunk_seg_times.append(seg_time)\n",
    "    chunk_total_times.append(detection_time + seg_time)\n",
    "\n",
    "\n",
    "print(chunk_seg_times)\n",
    "print(chunk_total_times)\n",
    "\n",
    "# After all chunks, compute dice/recall per view, if we use full body \n",
    "# (since we only currently have ground truth for full body)\n",
    "if mask_height_span > .8:\n",
    "    dice = 0\n",
    "    recall = 0\n",
    "    for v in range(num_views):\n",
    "        gt_mask_view = (masks[v]==2).to(torch.uint8)\n",
    "        dice_v = dice_score_batch(pred_mask_full[v].cpu(), gt_mask_view.cpu())\n",
    "        recall_v = recall_batch(pred_mask_full[v].cpu(), gt_mask_view.cpu())\n",
    "        dice += dice_v\n",
    "        recall += recall_v\n",
    "        print(f\"View {v}: Dice={dice_v:.4f}, Recall={recall_v:.4f}\")\n",
    "    print(dice/num_views)\n",
    "    print(recall/num_views)\n",
    "\n",
    "# Print timing info\n",
    "num_chunks = len(chunk_starts)\n",
    "avg_seg = sum(chunk_seg_times)/num_chunks\n",
    "avg_tot = sum(chunk_total_times)/num_chunks\n",
    "print(f\"Avg segmentation time per chunk = {avg_seg:.4f}s\")\n",
    "print(f\"Avg total time per chunk = {avg_tot:.4f}s\")\n",
    "print(f\"Total time for all chunks = {sum(chunk_total_times):.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eecbf03-3d50-4ac1-a290-344ad7c255c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(pred_mask_full[v].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8491da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iot-class/Capstone/3d-privacy-masking/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from simple_lama_inpainting import SimpleLama\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import sys\n",
    "# from toonify_image import load_toonify_model, toonify_image_with_stylegan # this will need tweaking based on paths\n",
    "\n",
    "def anonymize_region(img_np: np.ndarray, bool_mask: np.ndarray, method: str = \"color_noise\", noise_level: int = 100, pixel_size: int = 8, blur_kernel: int = 25, model_extra = None) -> np.ndarray:\n",
    "    \n",
    "    masked_img = img_np.copy()\n",
    "    H, W, _ = img_np.shape\n",
    "\n",
    "    if method == \"color_noise\":\n",
    "        img_int = img_np.astype(np.int16)\n",
    "        noise = np.random.randint(-noise_level, noise_level + 1, size=(H, W, 3), dtype=np.int16)\n",
    "        noise_masked = noise * bool_mask[..., None]\n",
    "        perturbed = np.clip(img_int + noise_masked, 0, 255).astype(np.uint8)\n",
    "        masked_img[bool_mask] = perturbed[bool_mask]\n",
    "        blurred = cv2.GaussianBlur(masked_img, (7, 7), 0)\n",
    "        anonymized_img = blurred\n",
    "\n",
    "\n",
    "    elif method == \"blur\":\n",
    "        blurred = cv2.GaussianBlur(img_np, (blur_kernel, blur_kernel), 0)\n",
    "        anonymized_img = blurred\n",
    "\n",
    "    elif method == \"random_rgb\":\n",
    "        random_colors = np.random.randint(0, 256, size=(H, W, 3), dtype=np.uint8)\n",
    "        anonymized_img = random_colors\n",
    "\n",
    "    elif method == \"color\":\n",
    "        red_overlay = np.zeros_like(img_np, dtype=np.uint8)\n",
    "        red_overlay[..., 0] = 255  # R\n",
    "        red_overlay[..., 1] = 0\n",
    "        red_overlay[..., 2] = 0\n",
    "\n",
    "        anonymized_img = red_overlay\n",
    "\n",
    "        \n",
    "    elif method == \"sampling\":\n",
    "        original_height, original_width = img_np.shape[:2]\n",
    "        downsampled = cv2.resize(img_np, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "        anonymized_img = cv2.resize(downsampled, (original_width, original_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    elif method == \"pixelate\":\n",
    "        small = cv2.resize(img_np, (W // pixel_size, H // pixel_size), interpolation=cv2.INTER_LINEAR)\n",
    "        anonymized_img = cv2.resize(small, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    elif method == 'all':\n",
    "        downsample_size=96\n",
    "        noise_std=5\n",
    "        matrix_strength=0.015\n",
    "        \n",
    "        h, w = img_np.shape[:2]\n",
    "\n",
    "        # 1. Fast lossy compression: downsample and upsample once\n",
    "        img_small = cv2.resize(img_np, (downsample_size, downsample_size), interpolation=cv2.INTER_AREA)\n",
    "        img_resized = cv2.resize(img_small, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        # 2. Add uniform noise (fast, no float32)\n",
    "        noise = np.random.randint(-noise_std, noise_std + 1, img_np.shape, dtype=np.int16)\n",
    "        noisy = np.clip(img_resized.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # 3. Lightweight random color mix (uint8-safe)\n",
    "        color_matrix = np.eye(3) + matrix_strength * np.random.randn(3, 3)\n",
    "        color_matrix = np.clip(color_matrix, 0, 1).astype(np.float32)  # keep it subtle\n",
    "\n",
    "        reshaped = noisy.reshape(-1, 3).astype(np.float32)\n",
    "        transformed = reshaped @ color_matrix.T\n",
    "        anonymized_img = np.clip(transformed, 0, 255).astype(np.uint8).reshape(h, w, 3)\n",
    "\n",
    "\n",
    "\n",
    "    elif method == 'lama':\n",
    "        bin_mask = np.zeros(img_np.shape[:2], dtype=np.uint8)\n",
    "        bin_mask[bool_mask] = 255\n",
    "        masked_img = model_extra(img_np, bin_mask)\n",
    "\n",
    "        if isinstance(masked_img, Image.Image):\n",
    "            masked_img = np.array(masked_img)\n",
    "\n",
    "        anonymized_img = masked_img\n",
    "    \n",
    "    elif method == 'ssd':\n",
    "        pipe = model_extra\n",
    "\n",
    "        bin_mask = np.zeros(img_np.shape[:2], dtype=np.uint8)\n",
    "        bin_mask[bool_mask] = 255\n",
    "        prompt = \"face\"\n",
    "        image_pil = Image.fromarray(img_np).convert(\"RGB\")\n",
    "        mask_pil = Image.fromarray(bin_mask).convert(\"L\")\n",
    "\n",
    "        masked_img = pipe(prompt=prompt, image=image_pil, mask_image=mask_pil).images[0]\n",
    "\n",
    "        anonymized_img = masked_img\n",
    "\n",
    "    elif method == \"toon\":\n",
    "        loaded_toonify_model = model_extra\n",
    "        image_pil = Image.fromarray(img_np).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "        anonymized_img = toonify_image_with_stylegan(\n",
    "            input_image=image_pil,\n",
    "            loaded_model=loaded_toonify_model\n",
    "        )\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    return anonymized_img\n",
    "\n",
    "\n",
    "def anonymize_depth(original_depth_np, noise_strength=0.01, output_path=None):\n",
    "\n",
    "    # --- Add Gaussian Noise (as before) ---\n",
    "    original_depth_np = original_depth_np.copy()  # Ensure we don't modify the original data\n",
    "    noise_mean_gaussian = 0.0\n",
    "    noise_std_dev_gaussian = noise_strength # Example: 5 millimeters\n",
    "    gaussian_noise = np.random.normal(noise_mean_gaussian, noise_std_dev_gaussian, original_depth_np.shape)\n",
    "    # Apply Gaussian noise to the original masked_depth\n",
    "    noisy_depth_gaussian = original_depth_np + gaussian_noise\n",
    "    # --- End Add Gaussian Noise ---\n",
    "\n",
    "    # --- Add Uniform Random Noise (Very Quick) ---\n",
    "    uniform_noise_magnitude = noise_strength/2\n",
    "\n",
    "    # Generate uniform noise within the range [-magnitude/2, +magnitude/2]\n",
    "    uniform_noise = np.random.uniform(\n",
    "        low=-uniform_noise_magnitude / 2.0,\n",
    "        high=uniform_noise_magnitude / 2.0,\n",
    "        size=noisy_depth_gaussian.shape\n",
    "    )\n",
    "\n",
    "    # Add the uniform noise to the already Gaussian-noisy depth\n",
    "    return noisy_depth_gaussian + uniform_noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca63f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from simple_lama_inpainting import SimpleLama\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "# from toonify_image import load_toonify_model, toonify_image_with_stylegan\n",
    "\n",
    "\n",
    "\n",
    "PSP_MODEL_PATH = \"pretrained_models/psp_toonify.pt\"\n",
    "\n",
    "\n",
    "model_used = True\n",
    "model_used = False\n",
    "\n",
    "if model_used:\n",
    "    simple_lama = SimpleLama()\n",
    "\n",
    "    # Load stable_diffusion from Hugging Face\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-inpainting\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",  # necessary for speed\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # # Load SSD-1B from Hugging Face\n",
    "    # pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    #     \"\",\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     variant=\"fp16\",  # necessary for speed\n",
    "    # ).to(\"cuda\")\n",
    "\n",
    "    # load style GAN\n",
    "    loaded_toonify_model = load_toonify_model(PSP_MODEL_PATH)\n",
    "    print(\"Toonify model successfully loaded for the pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a999ea37-5815-4083-b087-d0f4e59e4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "final_img_full  = np.zeros(\n",
    "    (num_views, num_frames, images.shape[-2], images.shape[-1], 3), dtype=np.uint8\n",
    ")\n",
    "\n",
    "\n",
    "def save_masked_images(pred_mask_full, images, out_folder, dilation_radius=4):\n",
    "    \"\"\"\n",
    "    pred_mask_full: [V, F, H, W] torch.Tensor of 0/1 masks\n",
    "    images:         [V, F, C, H, W] torch.Tensor, C >= 4 (RGB + depth)\n",
    "    out_folder:     root output directory\n",
    "    dilation_radius: pixels to dilate each mask\n",
    "    \"\"\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    num_views, num_frames = pred_mask_full.shape[:2]\n",
    "\n",
    "    # create per-view subfolders\n",
    "    for v in range(num_views):\n",
    "        rgb_dir = os.path.join(out_folder, \"rgb\", f\"view{v}\")\n",
    "        depth_dir = os.path.join(out_folder, \"depth\", f\"view{v}\")\n",
    "        os.makedirs(rgb_dir, exist_ok=True)\n",
    "        os.makedirs(depth_dir, exist_ok=True)\n",
    "\n",
    "    # prepare dilation kernel\n",
    "    k = 2 * dilation_radius + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "\n",
    "    chunk_total_times = []\n",
    "    chunk_write_times = []\n",
    "    chunk_anonym_times = []\n",
    "\n",
    "    # process in chunks of X frames (assume X is defined elsewhere)\n",
    "    for start in range(0, num_frames, X):\n",
    "        end = min(start + X, num_frames)\n",
    "        t_chunk = time.time()\n",
    "\n",
    "        image_writes = []\n",
    "\n",
    "        for v in range(num_views):\n",
    "            rgb_dir   = os.path.join(out_folder, \"rgb\",   f\"view{v}\")\n",
    "            depth_dir = os.path.join(out_folder, \"depth\", f\"view{v}\")\n",
    "\n",
    "            for f in range(start, end):\n",
    "\n",
    "                # 1) dilate mask\n",
    "                mask_np = (\n",
    "                    pred_mask_full[v, f]\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    .astype(np.uint8)\n",
    "                )\n",
    "                mask = cv2.dilate(mask_np, kernel).astype(bool)\n",
    "\n",
    "                # 2) extract RGB\n",
    "                rgb = images[v, f, :3].detach().cpu().numpy()        # [3,H,W]\n",
    "                rgb = np.transpose(rgb, (1, 2, 0))                   # [H,W,3]\n",
    "                if rgb.dtype != np.uint8:\n",
    "                    rgb = (rgb * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "                # 3) extract depth\n",
    "                depth = images[v, f, 3].detach().cpu().numpy()       # [H,W]\n",
    "\n",
    "                # 4) anonymize once per chunk-start frame\n",
    "                if f == start:\n",
    "                    anon_rgb = anonymize_region(rgb, mask, method=\"all\")\n",
    "                    if isinstance(anon_rgb, Image.Image):\n",
    "                        anon_rgb = np.array(anon_rgb)\n",
    "                    if anon_rgb.shape != rgb.shape:\n",
    "                        anon_rgb = cv2.resize(\n",
    "                            anon_rgb,\n",
    "                            (rgb.shape[1], rgb.shape[0]),\n",
    "                            interpolation=cv2.INTER_LINEAR\n",
    "                        )\n",
    "                    anon_depth = anonymize_depth(depth, noise_strength=10)\n",
    "\n",
    "                # 5) apply anonymization\n",
    "                out_rgb = rgb.copy()\n",
    "                out_depth = depth.copy()\n",
    "                out_rgb[mask] = anon_rgb[mask]\n",
    "                out_depth[mask] = anon_depth[mask]\n",
    "\n",
    "                # 6) store in global array\n",
    "                final_img_full[v, f] = out_rgb\n",
    "\n",
    "                t_write = time.time()\n",
    "                # 7) save RGB mask\n",
    "                rgb_path = os.path.join(rgb_dir, f\"{f}_masked.png\")\n",
    "                cv2.imwrite(\n",
    "                    rgb_path,\n",
    "                    cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR)\n",
    "                )\n",
    "\n",
    "                # 8) save depth as 16-bit PNG\n",
    "                depth_path = os.path.join(depth_dir, f\"{f}_depth.png\")\n",
    "                depth_uint16 = np.clip(out_depth, 0, 20000).astype(np.uint16)\n",
    "                cv2.imwrite(depth_path, depth_uint16)\n",
    "                t_write_end = time.time()\n",
    "                \n",
    "                image_writes.append(time.time() - t_write)\n",
    "\n",
    "        total_chunk = time.time() - t_chunk\n",
    "        write_total = sum(image_writes)\n",
    "        chunk_total_times.append(total_chunk)\n",
    "        chunk_write_times.append(write_total)\n",
    "        chunk_anonym_times.append(total_chunk - write_total)\n",
    "\n",
    "    # 10) Print stats\n",
    "    n = len(chunk_starts)\n",
    "    avg_total = sum(chunk_total_times) / n\n",
    "    avg_write = sum(chunk_write_times)  / n\n",
    "    avg_anon  = sum(chunk_anonym_times) / n\n",
    "\n",
    "    print(f\"Avg total time per chunk = {avg_total:.4f}s\")\n",
    "    print(f\"Avg anonymization time per chunk = {avg_anon:.4f}s\")\n",
    "    print(f\"Total time for all chunks w/ IO = {sum(chunk_total_times):.4f}s\")\n",
    "    print(f\"Total time for all chunks anon only = {sum(chunk_anonym_times):.4f}s\")\n",
    "    print(f\"Saved masked images in folder: {out_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from simple_lama_inpainting import SimpleLama\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "final_img_full  = np.zeros(\n",
    "    (num_views, num_frames, images.shape[-2], images.shape[-1], 3), dtype=np.uint8\n",
    ")\n",
    "\n",
    "\n",
    "# assume X, num_views, num_frames, images and final_img_full are defined globally\n",
    "\n",
    "def save_masked_images_enhanced(pred_mask_full, images, out_folder, dilation_radius=4):\n",
    "    \"\"\"\n",
    "    Faster batched version to eliminate recursive calls and use parallel IO\n",
    "\n",
    "    pred_mask_full: [V, F, H, W] 0/1 masks (same mask for all frames in a chunk)\n",
    "    images:         [V, F, C, H, W]  (C>=4, with RGB + depth at index 3)\n",
    "    \"\"\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    V, F, H, W = pred_mask_full.shape[0], pred_mask_full.shape[1], images.shape[-2], images.shape[-1]\n",
    "\n",
    "    # 1) Move all data to CPU once\n",
    "    masks_np = pred_mask_full.detach().cpu().numpy().astype(np.uint8)   # [V, F, H, W]\n",
    "    imgs_np  = images.detach().cpu().numpy()                            # [V, F, C, H, W]\n",
    "\n",
    "    # 2) Prepare dilation kernel\n",
    "    k = 2 * dilation_radius + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "\n",
    "    # 3) Create per-view folders once\n",
    "    for v in range(V):\n",
    "        os.makedirs(os.path.join(out_folder, f\"rgb/view{v}\"),   exist_ok=True)\n",
    "        os.makedirs(os.path.join(out_folder, f\"depth/view{v}\"), exist_ok=True)\n",
    "\n",
    "    chunk_anonym_times = []\n",
    "    chunk_write_times  = []\n",
    "    chunk_total_times  = []\n",
    "\n",
    "    chunk_starts = list(range(0, F, X))\n",
    "    for start_f in chunk_starts:\n",
    "        end_f = min(start_f + X, F)\n",
    "        t_chunk = time.time()\n",
    "        image_writes = []\n",
    "\n",
    "        for v in range(V):\n",
    "            # 4) Slice out this chunk once\n",
    "            chunk_imgs  = imgs_np[v, start_f:end_f]       # [K, C, H, W]\n",
    "            rgb_chunk   = chunk_imgs[:, :3]               # [K, 3, H, W]\n",
    "            depth_chunk = chunk_imgs[:, 3]                # [K, H, W]\n",
    "\n",
    "            # 5) Convert RGB to HWC uint8\n",
    "            rgb_uint8 = (rgb_chunk.transpose(0, 2, 3, 1) * 255).clip(0, 255).astype(np.uint8)  # [K, H, W, 3]\n",
    "\n",
    "            # 6) Compute & dilate mask once per view/chunk\n",
    "            mask2d    = cv2.dilate(masks_np[v, start_f], kernel)  # assume masks_np[v, f] is identical for f in chunk\n",
    "            bool_mask = mask2d.astype(bool)\n",
    "\n",
    "            # 7) Compute anonymization once\n",
    "            anon_img   = anonymize_region(rgb_uint8[0], bool_mask, method=\"all\")\n",
    "            # anon_img   = anonymize_region(rgb_uint8[0], bool_mask, method=\"color\")\n",
    "            if isinstance(anon_img, Image.Image):\n",
    "                anon_img = np.array(anon_img)\n",
    "            if anon_img.shape[:2] != (H, W):\n",
    "                anon_img = cv2.resize(anon_img, (W, H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            anon_depth = anonymize_depth(original_depth_np=depth_chunk[0], noise_strength=10)\n",
    "\n",
    "            # 8) Apply mask to all frames at once\n",
    "            masked_rgb   = rgb_uint8.copy()    # [K, H, W, 3]\n",
    "            masked_rgb[:, bool_mask] = anon_img[bool_mask]\n",
    "\n",
    "            masked_depth = depth_chunk.copy()   # [K, H, W]\n",
    "            masked_depth[:, bool_mask] = anon_depth[bool_mask]\n",
    "\n",
    "            t_write = time.time()\n",
    "            # 9) Save each frame in parallel to overlap IO\n",
    "            def _save(idx, frame_rgb, frame_depth):\n",
    "                f = start_f + idx\n",
    "                final_img_full[v, f] = frame_rgb\n",
    "                bgr = frame_rgb[:, :, ::-1]\n",
    "                cv2.imwrite(os.path.join(out_folder, f\"rgb/view{v}/{f}_masked.png\"), bgr)\n",
    "                depth_u16 = np.clip(frame_depth, 0, 20000).astype(np.uint16)\n",
    "                cv2.imwrite(os.path.join(out_folder, f\"depth/view{v}/{f}_depth.png\"), depth_u16)\n",
    "\n",
    "            with ThreadPoolExecutor() as exe:\n",
    "                for i in range(end_f - start_f):\n",
    "                    exe.submit(_save, i, masked_rgb[i], masked_depth[i])\n",
    "            total_write = time.time() - t_write\n",
    "            image_writes.append(total_write)\n",
    "            \n",
    "\n",
    "        total_chunk = time.time() - t_chunk\n",
    "        write_total = sum(image_writes)\n",
    "        chunk_total_times.append(total_chunk)\n",
    "        chunk_write_times.append(write_total)\n",
    "        chunk_anonym_times.append(total_chunk - write_total)\n",
    "\n",
    "    # 10) Print stats\n",
    "    n = len(chunk_starts)\n",
    "    avg_total = sum(chunk_total_times) / n\n",
    "    avg_write = sum(chunk_write_times)  / n\n",
    "    avg_anon  = sum(chunk_anonym_times) / n\n",
    "\n",
    "    print(f\"Avg total time per chunk = {avg_total:.4f}s\")\n",
    "    print(f\"Avg anonymization time per chunk = {avg_anon:.4f}s\")\n",
    "    print(f\"Total time for all chunks w/ IO = {sum(chunk_total_times):.4f}s\")\n",
    "    print(f\"Total time for all chunks anon only = {sum(chunk_anonym_times):.4f}s\")\n",
    "    print(f\"Saved masked images in folder: {out_folder}\")\n",
    "\n",
    "    return avg_anon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb79be-015a-4807-9e1a-0eacf76be0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg total time per chunk = 0.1743s\n",
      "Avg anonymization time per chunk = 0.0813s\n",
      "Total time for all chunks w/ IO = 4.3585s\n",
      "Total time for all chunks anon only = 2.0322s\n",
      "Saved masked images in folder: jc_8_long_n2\n"
     ]
    }
   ],
   "source": [
    "final_img_full  = np.zeros(\n",
    "    (num_views, num_frames, images.shape[-2], images.shape[-1], 3), dtype=np.uint8\n",
    ")\n",
    "\n",
    "\n",
    "# avg_anon = save_masked_images(pred_mask_full, images, output_base_directory)\n",
    "avg_anon = save_masked_images_enhanced(pred_mask_full, images, output_base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c31f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving annotated images to: /home/iot-class/Capstone/3d-privacy-masking/h_results/jc_8_long_n2\n",
      "\n",
      "Image saving process complete.\n"
     ]
    }
   ],
   "source": [
    "# Create the base directory if it doesn't exist\n",
    "os.makedirs(output_base_directory, exist_ok=True)\n",
    "print(f\"\\nSaving annotated images to: {os.path.abspath(output_base_directory)}\")\n",
    "\n",
    "for v in range(num_views):\n",
    "    view_directory = os.path.join(output_base_directory, f\"rgb/view{v}\")\n",
    "    os.makedirs(view_directory, exist_ok=True) # Create a subdirectory for each view\n",
    "\n",
    "    for f in range(num_frames):\n",
    "        img_bounding_boxes = detect_objects(model, final_img_full[v,f], DETECTION_CONFIDENCE_THRESHOLD, True)\n",
    "        filename = os.path.join(view_directory, f\"{f}_detect.png\") # :03d for zero-padding frame number\n",
    "        \n",
    "\n",
    "        success = cv2.imwrite(filename, cv2.cvtColor(img_bounding_boxes, cv2.COLOR_RGB2BGR))\n",
    "        if success:\n",
    "            pass # Keep it clean for many files\n",
    "        else:\n",
    "            print(f\"Failed to save {filename}\")\n",
    "\n",
    "print(\"\\nImage saving process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55031995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FPS: 15.68 frames per second\n"
     ]
    }
   ],
   "source": [
    "fps = 1/((avg_anon + avg_seg)/ X)\n",
    "print(f\"Estimated FPS: {fps:.2f} frames per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b117b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to /home/iot-class/Capstone/3d-privacy-masking/h_results/jc_8_long_n2/videos/view0.mp4\n",
      "Video saved to /home/iot-class/Capstone/3d-privacy-masking/h_results/jc_8_long_n2/videos/view1.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Set parameters\n",
    "num_frames = num_frames  # Already defined in your notebook\n",
    "\n",
    "os.makedirs(video_output_dir, exist_ok=True)\n",
    "for view_idx in range(num_views):\n",
    "    output_path = os.path.join(video_output_dir, f\"view{view_idx}.mp4\")\n",
    "    \n",
    "    input_video_pattern = input_video_base_path + f\"{view_idx}/{{}}_masked.png\"\n",
    "\n",
    "    # Read the first frame to get the size\n",
    "    first_frame_path = input_video_pattern.format(0)\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    if first_frame is None:\n",
    "        raise FileNotFoundError(f\"First frame not found: {first_frame_path}\")\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    # Define the video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write frames to video\n",
    "    for i in range(num_frames):\n",
    "        frame_path = input_video_pattern.format(i)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Frame not found: {frame_path}, skipping.\")\n",
    "            continue\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
